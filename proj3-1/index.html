<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
  figcaption {
    color: blue;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Lena Kushigemachi and Charles Liu</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://charlesliu02.github.io/proj-webpage-clk/proj3-1/index.html">https://charlesliu02.github.io/proj-webpage-clk/proj3-1/index.html</a></h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p style="color: blue;">
  In this project, we implemented a path tracer engine that is capable of rendering scenes with direct and indirect lighting while using BVH acceleration and adaptive sampling. For our ray generation and scene intersection implementations, we followed the formula for converting image coordinates to world space, the Möller Trumbore algorithm, and the formula for ray-sphere intersections. Some bugs that we encountered were not using the dot function in the Möller Trumbore algorithm and not checking for valid \(t\)s when checking ray-sphere intersections. For our BVH implementation, we recursively constructed a BVH. A bug that we encountered was infinite recursion, which occured when our heuristic was unable to split our group of primitives into smaller groups. We fixed this by manually splitting the group. Some bugs we encountered in our direct and indirect illumination implementations was not correctly checking for <i>max_ray_depth</i> and not correctly calculating the total reflectance. Our adaptive sampling implementation follows the formula \(I = 1.96 * sigma/sqrt(n)\), which was specified in the project specs.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p style="color: blue;">
    Given image coordinates, we transform them to a ray in world space by mapping the image coordinates from image space to sensor camera space, creating a camera ray. Then, we convert the ray in camera space to a ray in world space. To map the image coordinates from image space to camera space, we first subtract \(x\) and \(y\) by \(0.5\) to account for the shift and then multiply \(x\) by \(2 * tan(0.5 * hFov)\) and \(y\) by \(2 * tan(0.5 * vFov)\) to scale the coordinates appropriately. Then, we initialize a new ray with origin starting at the camera's origin and direction pointed at the coordinates in camera space transformed to the world space. We multiply the camera position and camera space coordinates by the camera to world matrix, \(c2w\), to transform them to the world space. We also set the ray's \(min_t = nClip\) and \(max_t = fClip\).
</p>
<p style="color: blue;">
  Now that we are able to generate rays, we are able to render scenes by generating <i>ns_aa</i> rays per pixel, which is the number of camera rays per pixel specified by the pathtracer, finding the radiance emitted per ray, and finding the Monte Carlo estimate of the pixel. Then, we update <i>sampleBuffer</i> at the corresponding pixel with the estimated value.
</p>
<p style="color: blue;">
  For ray-triangle intersection, we utilized the Möller Trumbore algorithm, which detects if a ray intersects with a triangle in 3d space. We also used barycentric coordinates, where the alpha, beta, and gamma values were generated by the Möller Trumbore algorithm, in order to calculate the correct surface normal at the intersection point.
</p>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part1/moller_trumbore.png" align="middle" width="400px"/>
        <figcaption>Möller Trumbore Algorithm</figcaption>
      </td>
    </tr>
  </table>
</div>
<p style="color: blue;">
  For ray-sphere intersection, we set the equations of a ray and a sphere to be equal to each other and solved for \(t\), the time when the ray intersects with the sphere, using the quadratic formula. If \(t\) doesn't exist, then the ray doesn't intersect with the sphere. If the ray intersects with the sphere twice, then we take note of the smaller \(t\) because we only care about when the ray intersects with the sphere for the first time. We make sure to set \(t_1\) and \(t_2\) appropriately where \(t_1\) &lt= \(t_2\) and that both of the values are valid.
</p>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part1/ray-sphere.png" align="middle" width="400px"/>
        <figcaption>Equation for ray-sphere intersection</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p style="color: blue;">
    The triangle intersection algorithm we implemented utilizes the fact that a point on a triangle can be found using barycentric coordinates, which we set equal to the ray equation. This gives us a system of linear equations, when solved, gives us the time the ray intersects with the triangle and the barycentric coordinates for the point on the triangle. We can rearrange the equation and utilize Cramer's rule to help solve for \(t\) and the barycentric coordinates.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part1/cube.png" align="middle" width="400px"/>
        <figcaption>cube.dae>
      </td>
      <td>
        <img src="images/Part1/beetle.png" align="middle" width="400px"/>
        <figcaption>beetle.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part1/teapot.png" align="middle" width="400px"/>
        <figcaption>teapot.dae</figcaption>
      </td>
      <td>
        <img src="images/Part1/cow.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p style="color: blue;">
  Our BVH construction algorithm is a recursive function that takes a list of Primitives and a constant, <i>max_leaf_size</i>, which represents how many Primitives will be represented by each <i>BVHNode</i> leaf node. It does the following:
    <ul style="color: blue;">
      <li><b>Base case:</b> Check if <i>size(list of primitives)</i> &lt= <i>max_leaf_size</i>. If it is, create a new <i>BVHNode</i> object and iterate through the primitives to create a boundary box that encompasses all of the primitives using the \(expand\) function. We also set the \(start\) and \(end\) fields of the node to its respective values</li> 
      <li><b>Recursive case:</b> The heuristic we used to determine how to split the list of primitives into left and right groups was based on the axis with the largest total variance. We first calculated which axis to split on by finding the total variance on each axis. The total variance can be computed by summing the variance, the distance from the centroid to the average centroid position squared of each individual bounding box. Once we have chosen an axis, we split the primitives into left and right groups. If a primitive's bounding box's centroid was less than the average, then it was put in the left group, and vice versa for the right group. Then, we return a <i>BVHNode</i> with left and right values set to the recursive calls of <i>construct_bvh</i> on the left and right groups. We had to manually check for infinite recursion, which could happen if the left or right group is empty, but the number of primitives left is still larger than <i>max_leaf_size</i>. If that is the case, then we manually add <i>max_leaf_size</i> primitives to the right group.
    </ul>
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part2/beast.png" align="middle" width="400px"/>
        <figcaption>beast.dae</figcaption>
      </td>
      <td>
        <img src="images/Part2/blob.png" align="middle" width="400px"/>
        <figcaption>blob.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part2/dragon.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
      <td>
        <img src="images/Part2/wall-e.png" align="middle" width="400px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part2/cow.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part2/cow_info.png" align="middle" width="400px"/>
        <figcaption>Rendering stats for cow.dae without BVH acceleration</figcaption>
      </td>
      <td>
        <img src="images/Part2/cow_bvh_info.png" align="middle" width="400px"/>
        <figcaption>Rendering stats for cow.dae with BVH acceleration</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part2/beetle.png" align="middle" width="400px"/>
        <figcaption>beetle.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part2/beetle_info.png" align="middle" width="400px"/>
        <figcaption>Rendering stats for beetle.dae without BVH acceleration</figcaption>
      </td>
      <td>
        <img src="images/Part2/beetle_bvh_info.png" align="middle" width="400px"/>
        <figcaption>Rendering stats for beetle.dae with BVH acceleration</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part2/CBbunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part2/CBbunny_info.png" align="middle" width="400px"/>
        <figcaption>Rendering stats for CBbunny.dae without BVH acceleration</figcaption>
      </td>
      <td>
        <img src="images/Part2/CBbunny_bvh_info.png" align="middle" width="400px"/>
        <figcaption>Rendering stats for CBbunny.dae with BVH acceleration</figcaption>
      </td>
    </tr>
  </table>
</div>
<p style="color: blue;">
    From rendering the three scenes above, we can see that rendering without BVH acceleration takes much longer than rendering with it. When using BVH acceleration, the total rendering time decreases, the average speed decreases, and the number of intersection tests per ray decreases. This leads to a ~121x speedup for cow.dae, ~344x speedup for beetle.dae, and ~2480x speedup for CBbunny.dae. The amount of speedup increases as the geometric complexity of the scene increases.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p style="color: blue;">
  <b>Uniform Hemisphere Sampling:</b> We implemented uniform hemisphere sampling with the following steps:
  </br>
  For each pixel:
  <ul style="color: blue;">
    <li>Take <i>(num of scene lights) * (num of samples per area light)</i> uniform samples by calling <i>get_sample()</i>. For each sample, \(w_j\):
      <ul>
        <li>Create a new ray with origin at the intersection point of the original ray pointed at \(o2w * w_j\). We convert \(w_j\) from object space to world space and set the ray's <i>min_t</i> value to be <i>EPS_F</i>.</li>
        <li>Then, we check if this new ray intersects with any objects in order to get the irradiance. If it does intersect, that means there will be light falling on the original intersection point, which we can find the value of by calling <i>get_emission</i>. If it doesn't then we don't include the sample in the final reflectance value.</li>
        <li>Assuming the object has irradiance, we calculate its radiance by calling <i>bsdf->f(\(w_j\), w_out)</i>, which will return the radiance of the surface at the given enter and exit angle. </li>
        <li>Then, we calculate the total with the following formula: <i>(radiance * irradiance * cos_theta(\(w_j\)) / (1 / 2pi)</i>.</li>
      </ul>
    </li>
  <li>Finally, we take the average of all of these samples and return it.</li>
  </ul>
</p>
<p style="color: blue;">
  <b>Lighting (Importance) Sampling:</b> We implemented lighting (importance) sampling with the following steps:
  <ul style="color: blue;">
    <li>For each scene light:
    </br>
      <ul>
        <li>If the scene light is a point light, then we do the same as below except we don't have to sample <i>ns_area_light</i> times because each sample will produce the same value. Instead we multiply the final value by <i>ns_area_light</i>.</li>
        <li>Call <i>sample_L, ns_area_light</i> times, giving us the light coming from the scene light. <i>sample_L</i> also gives us the sampled direction between our intersection point and the light source, the distance to the light source, and the probability of sampling the direction. We use these values to help us calculate the total reflectance. </li>
        <li>Then, for each sample, we check if a ray casted from the intersection point to the sampled direction intersects with any objects. If it does, then we don't include the sample in our lighting calculations, indicating that it is a shadow ray because it doesn't hit the light source. If it doesn't, then we call <i>bsdf->f(w2o * \(w_i\), w_out)</i> to find the radiance at the original intersection point.
        </li>
        <li>Then, we calculate the total reflectance at the current point with the following formula: <i>(radiance * irradiance * cos_theta(\(w_j\)) / pdf</i> where irradiance is the light coming from the scene light, the result of <i>sample_L</i>. pdf is the probability of sampling the direction, which is also given by <i>sample_L</i>.</li>
      </ul>
    </li>
    <li>Finally, we take the average of all of these samples and return it.</li>
  </ul>
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/Part3/CBbunny_H_16_8.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/Part3/CBbunny_16_8.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/Part3/CBspheres_lambertian_H_16_8.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/Part3/CBspheres_lambertian_16_8.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part3/wall-e_1_1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part3/wall-e_1_4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (wall-e.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part3/wall-e_1_16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part3/wall-e_1_64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (wall-e.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p style="color: blue;">
    As the number of light rays increases, the amount of noise decreases. In the 1 light ray image, there are visible holes in the Wall-E model. As the number of light rays increase, the model starts to appear more and more solid because for each pixel, we are sampling more rays. More samples leads to more rays hitting the light source, therefore rendering the model with enough light to be seen.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p style="color: blue;">
  Overall, lighting (importance) sampling produces images with a lot less noise than uniform hemisphere sampling because the rays that are sampled are pointing at a light source, whereas in uniform hemisphere sampling, the rays are pointed in random directions. Importance sampling is more accurate and allows us to actually sample and calculate the values that will be helpful in determining the reflectance of an object. It also weighs the samples based on their probabilities, placing higher emphasis on values with higher probabilities, which helps us render realistic scenes. Uniform hemisphere sampling produces more noise because sometimes the samples generated will not be casted in the direction of a light source.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p style="color: blue;">
    Our implementation of indirect lighting is similar to our implementation of direct lighting in that it follows the same process as checking for intersections and then calculating the final reflectance using the calculated radiance and irradiance. However, it differs because it recursive calculates the reflectance based on how many bounces the ray makes.
</p>
<p style="color: blue;">
  We implemented indirect lighting with the following steps:
  <ul style="color: blue;">
    <li>Calculate how much light is coming from a light source by calling <i>one_bounce_radiance.</i>
    </li>
    <li>Calculate the radiance at the intersection point by calling <i>sample_f</i>, retrieving a sampled vector giving the incoming radiance direction and a probability for the sample.
    </li>
    <li>Then, we create a new ray with origin, <i>hit_p</i> and direction <i>w2o * w_in</i>, setting it's <i>min_t</i> value to <i>EPS_F</i> and setting <i>ray.depth = r.depth - 1</i>. The \(depth\) field is used to make sure the ray doesn't bounce more than <i>max_ray_depth</i> times.
    </li>
    <li>Then, we terminate our recursion and return the light from the light source if we have hit the maximum number of ray bounces or if our Russion Roulette function returns true.
    </li>
    <li>If we don't terminate, we check if our new ray intersects with any other objects. If it does, then we recurse on that intersection and return the light from the light source multiplied with the radiance at the current point multiplied with the result of the recursion multiplied by cosine theta of our sampled vector divided by the pdf and the probability of bouncing. If it doesn't, then we just return the the light from the light source. The recursive equation takes on the following form: <i>(one_bounce_radiance + sample_f * at_least_one_bounce_radiance / pdf / probability_of_bouncing)</i>.
  </ul>
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/CBbunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/Part4/CBspheres_lambertian.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
    </tr>
    <tr align="center">
      </td>
      <td>
        <img src="images/Part4/dragon.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/CBspheres_lambertian_direct.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/CBspheres_lambertian_indirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p style="color: blue;">
    In the direct illumination only scene, only the light from the area light on the ceiling is being rendered. As a result, only pixels with direct rays to the light source will be illuminated. For example, the ceiling and parts of the speheres not facing the ceiling are all dark because they do not face the light. The top of the spheres and the walls are illuminated because they are not blocked by anything else. In contrast, the light in the indirect illumination only scene comes from subsequent bounces of the direct area light. As a result, the area light itself and the tops of the spheres are not as illuminated because there is no direct lighting on it. However, the undersides of the spheres and the ceiling are illuminated because the bounces of the direct light illuminate them.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/CBbunny_0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/CBbunny_1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/CBbunny_2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/CBbunny_3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/CBbunny_100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p style="color: blue;">
    For the \(m = 0\) scene, only the area light is illuminated because there are no bounces. For the \(m = 1\) scene, there is one bounce so parts of the scene that have a direct ray to the area light are illuminated and every other pixel blocked by another object is dark. For the \(m = 2\) scene, dark parts of the \(m = 1\) scene are now illuminated because secondary bounces of the direct light illuminate areas that do not have direct lighting from the area light. The \(m = 3\) and the \(m = 100\) scenes are almost identical, which might indicate that the illuminance of the pixels in the scene have converged.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/wall-e_1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/wall-e_2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (wall-e.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/wall-e_4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/wall-e_8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (wall-e.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/wall-e_16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/wall-e_64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (wall-e.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/wall-e_1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (wall-e.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p style="color: blue;">
  As the number of samples per pixels increase, the image becomes less noisy because there are more samples to accurately estimate the lighting in the scene. There is a higher number of samples that detect the lighting. Thus, when a Monte Carlo estimate is made, it is closer to the actual value. Additionally, the images look smoother because the samples average out more. However, the rendering time also increases because it takes more time to sample more rays and compute the calculations needed.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p style="color: blue;">
    Adaptive sampling is a technique used to eliminate the need for a large number of samples per pixel when a pixel has already converged. Some pixels converge at a lower sampling rate. As a result, we don't have to needlessly sample them if they have already converged. This allows us to save time and calculations and speed up our renders. Our implementation of adaptive sampling does the following:
</p>
<ul style="color: blue;">
  <li>Keep a running sum of the illuminance, \(s1\), and the illuminance squared, \(s2\) and keep track of the number of samples so far, \(n\).</li>
  <li>For every batch of samples, specified by \(samplesPerBatch\), find the mean, \(mu = s1 / n\), and variance, \(sigma = 1 / (n - 1) * (s2 - s1^2 / n)\).</li>
  <li>Then, calculate \(I = 1.96 * sigma / sqrt(n)\). If \(I &lt= maxTolerance * mu\), then the pixel has converged and we do not need any more samples.</li>
  <li>After sampling, we return the \(totalRadiance / n\).</li>
</ul>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part5/CBbunny.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part5/CBbunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part5/wall-e.png" align="middle" width="400px"/>
        <figcaption>Rendered image (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part5/wall-e_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (wall-e.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
